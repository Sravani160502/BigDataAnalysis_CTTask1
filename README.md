**COMPANY** : CODTECH IT SOLUTIONS  
**NAME** : V Sravani  
**INTERN ID** : CT12WVZG  
**DOMAIN** : Data Analytics  
**DURATION** : 12 weeks  
**MENTOR** : Neela Santhosh Kumar  


Task Description : 
For my internship task1 at CODTECH, I performed a comprehensive analysis on a large-scale bank customer dataset containing over 45,000 records and 17 features using PySpark to demonstrate scalability and efficient handling of big data. I began by setting up a Spark session and loading the dataset, followed by an initial exploratory data analysis (EDA) where I examined the schema, previewed the data, and generated descriptive statistics to understand the overall structure and distribution of features. I then investigated missing values across all columns to assess data quality. To gain insights into customer demographics and behaviors, I conducted group-wise aggregations such as counting customers by job type, calculating average account balances by education level, and determining the distribution of marital status with percentages. Further, I created an age grouping feature to categorize customers into meaningful segments (<30, 30-49, 50+), which helped in analyzing response rates to marketing campaigns by age group. I also explored the relationship between housing loan status and average balance, as well as the subscription rates for term deposits based on marital status, converting the target variable ‘y’ into a numeric response for quantitative analysis. Using PySpark’s filtering capabilities, I identified data patterns such as customers with unusually high call durations or campaign contacts. For visualization, I converted aggregate results to Pandas DataFrames to leverage Matplotlib and Seaborn libraries, generating bar plots to display campaign success rates across age groups and boxplots to examine balance distribution across different job categories, thus providing visual confirmation of trends observed in the data. Additionally, I performed data preprocessing steps necessary for machine learning by transforming categorical target labels into numeric format with StringIndexer, assembling selected numerical features into a vector suitable for model input, and splitting the data into training and testing sets to ensure unbiased evaluation. I trained a logistic regression model to predict customer subscription to term deposits, validating its accuracy on the test set. This project enabled me to gain hands-on experience with PySpark’s capabilities in handling large datasets efficiently, performing scalable exploratory data analysis, creating insightful visualizations, and building predictive models in a distributed computing environment, thus demonstrating proficiency in big data analytics that aligns with industry standards for scalability and performance.

**OUTPUT(some of them)**

![Image](https://github.com/user-attachments/assets/5d801ca6-8078-482a-a264-6832b61b59c3)

![Image](https://github.com/user-attachments/assets/0c3e213e-56dc-4dae-bfcb-347b04dd09a1)

![Image](https://github.com/user-attachments/assets/631ddf76-952c-4c3f-a378-240bae3f4c85)

![Image](https://github.com/user-attachments/assets/a96ebb1c-ce12-4893-8f45-a15d140bdca3)
